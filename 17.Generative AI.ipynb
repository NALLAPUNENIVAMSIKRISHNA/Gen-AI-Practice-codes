{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4ea2e5a2-8455-4c8f-b90f-b1a9b3102b8a",
   "metadata": {},
   "source": [
    "                    AI Vs ML Vs DL VS Gen AI\n",
    "Main is AI it is build app's that can perform it's own tasks without human intervention\n",
    "Ex:- Netflix - Recommendation system, Self driving car.\n",
    "Eod have to integrate with some web app or android app or any devices.\n",
    "\n",
    "ML is subset of AI, stats to perform all tasks, Main use of ai is to perform statistical analysis,visualization,prediction and forecasting.\n",
    "we understand about the data.\n",
    "\n",
    "DL is subset of ML, wast built to mimic human brain- Multilayered neural networks\n",
    "\n",
    "Deep learning:- \n",
    "    ANN - how basic ml is trained\n",
    "    CNN and object detection - computer vision ex:-rcnn n many more\n",
    "    RNN and it's variant - text related use cases, time series usecases ex:-rnn,lstm rnn,gru,encoder decoder,attention and transformer & bert from here only we will derive gen ai\n",
    "\n",
    "Gen ai is subset of DL\n",
    "                    Models\n",
    "              |                  |\n",
    "    Discriminative           Generative ai[LLM, LIM]\n",
    "    Tasks:-                   Tasks:-\n",
    "classification & prediction  Generate new data trained on huge dataset\n",
    "Dataset:- Trained on labeled \n",
    "dataset\n",
    "\n",
    "LLM MODELS:-\n",
    "Open ai , meta , google , anthropic\n",
    "gpt 4.0 , lama2,gemini , claude3                      these all are foundation models n pretrained models trained on huge data n fine tuning\n",
    "                                                       usecases are domain specific usecases\n",
    "\n",
    "RAG Applications - > chatbot's -> automation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "558e633a-0652-4dfa-8c3e-7ab12e806605",
   "metadata": {},
   "source": [
    "                        How chatgpt is trained [website,articles,books,etc]\n",
    "\n",
    "How chatgpt is trained :- https://www.linkedin.com/pulse/discover-how-chatgpt-istrained-pradeep-menon\n",
    "\n",
    "Internet data\n",
    "    |\n",
    "Stage 1: Generative pre training\n",
    "    |\n",
    "Basic gpt model\n",
    "    |\n",
    "Stage 2:Supervised fine tuning(SFT)\n",
    "    |\n",
    "Fine tuned chatgpt model\n",
    "    |\n",
    "Stage 3:Reinforcement learning HF\n",
    "    |\n",
    "Chatgpt model\n",
    "\n",
    "Stage 1:- Generative Pre Training\n",
    "Internet huge input text data -> Transformers -> Base gpt model -> Tasks:- Langugage translation,text summarisation,text completion,sentiment analysis\n",
    "                                                        |\n",
    "                                                    What we want -> Conversation - chatbot\n",
    "\n",
    "2.Supervised fine tuning(SFT):-\n",
    "Real conversation\n",
    "    Human1 & human 2 the conversation request and response is getting captured & then acting like a chatbot\n",
    "                                        |\n",
    "                            SFT Training data corpus\n",
    "Request-> Conversation history   Request response1      (Training)         SFT\n",
    "Response->But ideal response     Request response2 => Base gpt model -> Chatgpt model\n",
    "                                 Request response3          |\n",
    "                                                        Optimizer (Stochastic gradient descent model (SGD))\n",
    "\n",
    "3.Reinforcement learning through human feedback(RLHF):-\n",
    "    We will rank all the response and then create a reward model(binary classification - cross entropy) n then create a probability if pob is high it's good response\n",
    "after the reward model is created reinforcement is applied with proximal policy optimization\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc272389-867a-425f-9054-6005ad8a5186",
   "metadata": {},
   "source": [
    "Evolution of LLM's models\n",
    "Eliza(1967) -> SHRDLU 1970 -> XCALIBU 1980 -> RNN 1988 -> LSTM 1997 -> Transformers 2017 -> BERT,GPT 2018 -> GPT-2,RoBERT'S,XLNot 2019 -> GPT-3(OPEN AI) 2020->  GPT 3.5 2021 -> PALM,Instruct GPT 2022 -> LLaMa Falcon,GPT-4 LIMA,PaLM 2, BARD,Dolly 2,Guanaco 2023 ->GPT 4.0 MULTIMODEL - NLP,IMAGES, GOOLE GEMINI PRO,LAMA 3 , CLAUDE OPUS\n",
    "Google gemma2\n",
    "\n",
    "https://drive.google.com/file/d/1te0vEvvoyXXMmuhr9lKJf_74S6yJu4mg/view?usp=sharing\n",
    "https://drive.google.com/file/d/1wqNuiB_AwqAYO-fLu2b30XaAxEvEaibb/view?usp=sharing\n",
    "https://artificialanalysis.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
