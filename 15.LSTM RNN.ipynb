{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fee26862-dd49-445e-bd0e-c72c2f333a6c",
   "metadata": {},
   "source": [
    "LSTM RNN [Long short term memory]\n",
    "RNN -> Long term dependencies(not having in rnn) -> Vanishing gradient problem\n",
    "\n",
    "- problems of RNN\n",
    "- Why lstm rnn insted of rnn\n",
    "- How lstm rnn is solving the problem - what is long term and short term memory\n",
    "- Lstm Architecture\n",
    "- Working of Lstm with examples\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "- problems of RNN -> Long term dependncy\n",
    "        task is to predicting next word in sentence\n",
    "        The colour of the sky is ____ (answer is blue) we can solve this with rnn and we cannot also face vanishing gradien descent problem & here the gap is less and not that much dependent\n",
    "        I grew up in india.I speak fluent ____ (consider here we want to get one language which lang is it ? & it depends on india also but the gap is huge between those two) & there is a long term dependecy\n",
    "\n",
    "- Why lstm rnn insted of rnn\n",
    "        In rnn we are having short term memory but in Lstm rnn we are having both long and short term memory\n",
    "\n",
    "point wise operator\n",
    "    v1=[1 2 3]\n",
    "    v2=[4 5 6]\n",
    "* = [4 8 10]\n",
    "+ = [5 7 9]\n",
    "for v1 with tanh = [tanh(1) tanh(2) tahnh(3)]\n",
    "\n",
    "memory cell - stores long term memory(what i want to store for ling time)\n",
    "* is for removing context\n",
    "+ is for adding context\n",
    "c(t) is state for that particular time\n",
    "\n",
    "            forget gate\n",
    "memory c(t-1) -Straight line collecting all \n",
    "hidden gate H(t-1) Hidden state of previous time stamp\n",
    "X(t) - Word passed as i/p in the current time stamp\n",
    "    order of items\n",
    "        forget gate F(t)\n",
    "        Input gate I(t)\n",
    "        Candidate memory C(t)\n",
    "        Output gate \n",
    "\n",
    "Use case to show Forget gate - Text to get Next word\n",
    "\n",
    "Text                              Next word\n",
    "x11 x12 x13 x14                     y15\n",
    "\n",
    "x(t)=[0 2 4 1]  x(t+1)=[4 5 1 2]\n",
    "h(t-1)=[1 2 4] c(t-1)=[4 2 1]\n",
    "\n",
    "In memory cell \n",
    "C(t-1)-----(*)-------H(t-1)\n",
    "             /\n",
    "            /\n",
    "        Sigmoid (o o o ) connected from down all 7 to above 3 as (7*3); (1*7)*(7*3)=(1*3)\n",
    "             (o o o o o o o)\n",
    "\n",
    "1. C(t-1)= [6 8 9]*(0 0 0)=[0 0 0] - Removing all the previous context\n",
    "2. C(t-1)= [6 8 9]*(1 1 1)=[6 8 9]\n",
    "3. C(t-1)= [6 8 9]*(05 1 0.5)=(3 8 4.5)\n",
    "\n",
    "Sigmoid (Combining [H(t-1),X(t)] multily with W(f)) and then Adding Bias B(f)\n",
    "Conlusion :- Based on context this forget gate will let go some information or will not let go some information that is what is all about forget gate\n",
    "\n",
    "\n",
    "            Input gate and candidate memory\n",
    "I(t)=Sigmoid(W(i)*(H(t-1),X(t))+B(i))\n",
    "C(t)=Tanh(W(c)*(H(t-1),X(t))+B(c))\n",
    "\n",
    "Context-If any information needed to be added it the mempry\n",
    "C(t-1)=The information will be added\n",
    "\n",
    "Combination of forget,input and candidate--> C(t)= (F(t)*C(t-1)) + (I(t)*C(t))\n",
    "(F(t)*C(t-1))- Removing or forgetting some info(Context chaning or else same) \n",
    "(I(t)*C(t) -  Candidate memory task is to add new info(point wise operator)\n",
    "\n",
    "I stay in india... and i speak ____\n",
    "Saving india in context and forget some info neural netowrk is using to predict\n",
    "\n",
    "Forget gate & I/p gate (point wise operator *) Candidate memory) + C(t-1) = C(t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fffaec14-505a-4fbe-a757-02bfed2ab0e4",
   "metadata": {},
   "source": [
    "Output gate\n",
    "\n",
    "Forget gate - Forget some info based on context from the memory cell\n",
    "Input gate - Add info based on context\n",
    "\n",
    "O(t) = Sigmoid(W(O)[H(t-1,X(t))]+B(o))\n",
    "H(t)=O(t)*tanh(C(t))\n",
    "\n",
    "C(t)=Long term memory\n",
    "H(t) = Hidden state - Short term memory\n",
    "There is One memmory cell and having info that want in long tem and all the other are also updated wrt hiiden state and short term memory till Loss is less and also do Back propagation and forward propagation.\n",
    "[Wi,Wc,Wo] -> Updating <- Back propagation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab37d0d4-1960-460e-9a2a-57c4148103e3",
   "metadata": {},
   "source": [
    "                                Training data with LSTM RNN\n",
    "Text Paragraph                             O/P(Food is good or bad)       Forget gate           Input gate                Output gate\n",
    "I went to restaurant and order burger       1\n",
    "The burger looked tasty and crispy\n",
    "But burger is not good for health\n",
    "It has lot of fats,cholestrol\n",
    "But the burger was made with \n",
    "whey protein and only vegetables\n",
    "were used, so it was good\n",
    "\n",
    "word -> vectors -> Embedding layers\n",
    "Word2vec(3 dimensional vector)\n",
    "\n",
    "        [good]       [bad]      [Healthy] <- Blackbox\n",
    "Tasty     0.9         0.0          0.1      The burger looked tasty and crispy\n",
    "\n",
    "\n",
    "For every word we will give same vector at particular time stamp"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5b8a222-d080-4918-bfb4-7a33525d3bbb",
   "metadata": {},
   "source": [
    "                    Variants of LSTM RNN\n",
    "Connections -> From memory cell to Forget gate\n",
    "                                   I/P gate\n",
    "                                   O/P gate\n",
    "Peep hole connections:- through this architecture we let the gate layers look at the cell state\n",
    "\n",
    "    2.Another variation where we are coupling forget and input gaates\n",
    "-Instead of seperately deciding what to forget and what we should add new info, we make this decision together \n",
    "C t = F t * C t-1 + (1- F t) * C t\n",
    "\n",
    "Goal :- we only forget when we are going to i/p something in it's place\n",
    "we only i/p new values to the state when we forget something older\n",
    "\n",
    "                3.GRU(Gated recurrent unit)\n",
    "Zt=Update gate= sigmoid(Wz * [Ht-1,Xt])\n",
    "Rt=Reset gate=Sigmoid(Wr* [Ht-1,Xt])\n",
    "H-t=Temporary hidden state = tanh(W*[Rt*H t-1, X t])\n",
    "Ht=(1-Z t)*H t-1 + Z t* (H-t)\n",
    "\n",
    "Reset gate = Rt = Resetting some info from Ht-1 = Memory => Ltm + Stm\n",
    "\n",
    "H(t-1) = [0.6  0.5  0.3  0.9] * (point wise operator of these 2)\n",
    "R(t)   = [0.2  0.4  0.8  0.2]\n",
    "X(t)   = [0.12 0.20 0.24 0.18] <- Resetting <- Context\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3aef3862-105f-4dda-a49d-8b9db3475cdb",
   "metadata": {},
   "source": [
    "5.Lstm_rnn_p.ipynb --> Code (Project) in colab"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b2ae31e-fddd-4f8e-bf7b-a9ecfeafad03",
   "metadata": {},
   "source": [
    "                    Bidirectional RNN\n",
    "Types of rnn:- \n",
    "    -One to many rnn (Image captioning)\n",
    "    -Many to one rnn (image search)\n",
    "    -many to many rnn (lang translation)\n",
    "    -one to one rnn\n",
    "\n",
    "Example:- vamsi eats ____ in banglore.\n",
    "          vamsi eats ____ in paris\n",
    "With lstm,gru,simple rnn we only consider previous words but wrt Bidirectional rnn  it considers 2 sides\n",
    "\n",
    "https://drive.google.com/file/d/1b-EcJNsJjtgSJwmZcNPifLbLvGlDneZv/view?usp=sharing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a50b33ca-981f-4c93-9376-2125e1fd41fa",
   "metadata": {},
   "source": [
    "                Encoder and Decoder\n",
    "- Simple rnn -> vanishing gradient problem\n",
    "- Lstm rnn -> overcame gd problem and able to make long short term memory\n",
    "- Gru rnn -> overcame gd problem and able to make long short term memory\n",
    "- Bidirectional rnn - if there is furuther context wrt upcoming words\n",
    "\n",
    "Encoder and decoder:-\n",
    "ex:- one language to other\n",
    "english -> french\n",
    "ex:- linked in chat -> hi,how are you ? some suggestions like good , how are you ?  these all are seq 2 seq\n",
    "\n",
    "simple working of encoder and decoder:-\n",
    "    Input sentence-> Encoder(Converts words to array of numbers) -> hidden states get passed through -> Decoder -> the decoder generates the output word by word, and keeps feeding the previous word into the decoder again ->\n",
    "\n",
    "Input text -> Encoder -> context vector [0.1 0.8 -0.3 0.6 0.1](not horizontal vertical) -> Decoder -> summary(Output) \n",
    "\n",
    "Encoder\n",
    "\n",
    "OHE \n",
    "\n",
    "1000   0100   0010   0001 --> 100               010\n",
    "sos    thank  you    eos  --> sos              gracious               eos\n",
    "                              use softmax      use sofmax here \n",
    "\n",
    "https://drive.google.com/file/d/1uw2cFXZi998EdZvklYI2sNXFZ8STm4bO/view?usp=sharing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7449af2-b890-4501-be15-4fd30d384e2a",
   "metadata": {},
   "source": [
    "        Problems with encoder-decoder seq2seq architecture\n",
    "context vector represents the sentence(having more info of nearest time stamp wt abt t1)\n",
    "researchers tried with sentences of varying length\n",
    "blew score(accuracy) got decreased as the lenght of sentence increases\n",
    "\n",
    "paper-> neural machine translation by jointly learning to align and translate\n",
    "\n",
    "to overcome this attention mechanism -> seq2seq came"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5329a393-c679-4608-8da0-2d9586b43672",
   "metadata": {},
   "source": [
    "        Attention mechanism -> sequence to sequence\n",
    "for longer sentences with also the context vector and also the context is passed\n",
    "\n",
    "https://drive.google.com/file/d/1FHByfJm1db_UHWMaX8AUyb63GMGzYSs-/view?usp=sharing\n",
    "\n",
    "https://arxiv.org/pdf/1409.0473\n",
    "\n",
    "https://erdem.pl/2021/05/introduction-to-attention-mechanism"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
