{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP word embedding is a term used for the representation of words for text analysis,Typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say i am having Angry & happy and excited convert text to vector with pca \n",
    "Angry and happy are quite opposite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are specifically of 2 types\n",
    "Eod converting word to vectors or sentence to vectors\n",
    "\n",
    "Count or frequency               Deep learning trained models\n",
    "\n",
    "-One hot encoded                 -Word2vec\n",
    "-Bag of words                    \n",
    "-Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is of 2 types \n",
    "\n",
    "-Continous Bag of words\n",
    "-Skip Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec - Feature Representation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word2Vec is a technique for nlp publlished in 2013.The word2Vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detet synonymous words or suugests additional words for a partial sentence.As the name  implies, Word2vec represents each distinct word with a particular list of numbers called vector.\n",
    "\n",
    "Vocabulary -> Unique words->Corpus\n",
    "\n",
    "                Boy    Girl   King    Queen   Apple  Mango\n",
    "Gender          -1      1     -0.92    +0.93   0.01   0.23\n",
    "royal           0.01   0.02    0.95    0.96    -0.02  0.02\n",
    "age             0.03   0.02    0.75    0.68     0.95  0.96\n",
    "food             -     -       -         -      0.91  0.92\n",
    "...\n",
    "...\n",
    "\n",
    "King - Boy + Queen = Girl\n",
    "\n",
    "King [0.95,0.96]\n",
    "Queen [-0.96,0.95]\n",
    "Man [0.95,0.98]\n",
    "Women [-0.94,-0.96]\n",
    "\n",
    "King-Man+Queen=Women\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Distance = 1 - cosine similarity\n",
    "Cosine similarity = cos 45 = 1/sqrt(2) = 0.7071\n",
    "Distance=1-0.7071 = 0.29 (Some what similar)\n",
    "if 2 points are in x or y axis side by side they are same because of cos 0 =1\n",
    "Ditance = 1-1 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW(Continous Bag of Words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Corpus Dataset\n",
    "[Ineuron company is related to data science]\n",
    "window size = 5(Take odd number)\n",
    "\n",
    "       Input                                  Output\n",
    "- Ineuron,Company,Related,To                    Is\n",
    "- Company,Is,To,Data                            Related\n",
    "- Is,Related,Data,Science                       To\n",
    "\n",
    "One Hot Encoding(OHE)\n",
    "\n",
    "Ineuron [1000000]\n",
    "Company [0100000]\n",
    "Related [0001000]\n",
    "To      [0000100]\n",
    "\n",
    "CBOW (Fully Connected Neural Network)\n",
    "fig 1,2(Pictures of gen ai)\n",
    "\n",
    "Skip Gram\n",
    "Fig 3\n",
    "\n",
    "When should we apply cbow and skipgram\n",
    "\n",
    "Small dataset->Cbow\n",
    "Large dataset->Skipgram\n",
    "\n",
    "-Increase the training data\n",
    "-increase the window size which leads to increase in dimensions so better performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   Advantages of Word2Vec\n",
    "   \n",
    "-Sparse Matrix - Dense Matrix\n",
    "-Semantic info is getting captured \n",
    "-Vocabulary size - Fixed set of dimension  &   Google word2vec(300 dimensions)\n",
    "-Out of vocabulary is solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
