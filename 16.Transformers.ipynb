{
 "cells": [
  {
   "cell_type": "raw",
   "id": "36824fc8-8552-41d2-907b-a59d8ec200a0",
   "metadata": {},
   "source": [
    "    Introduction to transformers \n",
    "https://drive.google.com/file/d/1KWH4t6Ot92tlP6BmY2t6oQkf1Rtn7wIL/view?usp=sharing\n",
    "\n",
    "-RNN/LSTM/GRU RNN\n",
    "-Encoder Decoder Architecture\n",
    "-Attention mechanism\n",
    "-Transformers\n",
    "\n",
    "? Why transformers\n",
    "? Architecture of transformers\n",
    "? Self attention -> Query,Key,Value parameters\n",
    "? Positional Encoding\n",
    "? Multihead attention\n",
    "? Combining the working of transformers\n",
    "\n",
    "Generative ai -> LLM.Multimodel\n",
    "Bert, GPT-> Open ai -> chatgpt ->gpt 4.o\n",
    "\n",
    "What & why to use transformers ?\n",
    "Transformers in Natural language processing(NLP) are a type of deep learning model that use self-attention mechanism to analyze and process natural language data. They are encoder-decoder models that can be used for many applications,including machine translation -> seq2seq task\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Ex:- Language translation -> Google translation\n",
    "    English -> French\n",
    "    i/p many -> o/p many {Length of the sentence}\n",
    "\n",
    "1. Attention mechanism:- Parallely e cannot send all the words in a sentence -> scalable\n",
    "    Dataset-> huge -> Scalable wrt training\n",
    "    Transformers -> Lstm rnn\n",
    "                        /\n",
    "                        Self attention module -> all the words will be parallely sent to encoder\n",
    "\n",
    "Also learn Positional Encoding\n",
    "With Transformers -> Dataset (Increasing) -> Amazing;  State of art models <- Nlp\n",
    "Transfer learning -> multimodel task -> Nlp + image\n",
    "\n",
    "Transformers :- AI space -> SOTA model \n",
    "Like Bert and gpt -> Transfer learning -> Sota models -> Dalle\n",
    "They train with huge data\n",
    "\n",
    "2. Contextual Embedding :- Self attention model\n",
    "    Ex:- My name is vamsi and I play cricket. -> Sent to embedding layer - we will get some fixed vector for all words like (name,vamsi,and,...)\n",
    "now give same sentence to contextual vector embedding "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f165482e-f73c-4dd4-90a2-9cf6facc3659",
   "metadata": {},
   "source": [
    "                Basic architecture of encoder :- used to solve seq2seq task  -> Language translation (English to french)\n",
    "https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "what is inside encoder :- 1 self attention layer connected to 1 feed forward neural network\n",
    "what is inside decoder :- 1 self attention connected to encoder decoder attention and connected to feed forward\n",
    "\n",
    "Embedding layer(fixed vectors no context) will be sent first\n",
    "\n",
    "Encoder 1 - [1 self attention layer connected to 1 feed forward neural network in between this we are having contextual vectors for every word]\n",
    "Encoder 2 - [then sent to encoder 2 and the same sent to self attention layer and we are having contextual vectors created and then sent to feed forward neural network]\n",
    "\n",
    "Initially we are having transformer it is nothing but encoder and decoder architecture here in this encoder we are having multiple encoder (6 i research papaer) in encoder we are having self attention layer and feed forward neural network\n",
    "we don't use lstm & rnn insted we use self attention in encoder\n",
    "\n",
    "In the decoder self attention connected to encoder decoder attention and connected to feed forward\n",
    "with contextual vector when having large sentence we will get good accuracy\n",
    "next imp thing is we can pass all the words will be passed parallely"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2500fdf8-52c1-41b9-abaa-ed70a322a8e0",
   "metadata": {},
   "source": [
    "                Self attention at higher and detailed level\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "self attention also know as dot-product attention, is a crucuial mechanism in the transformer architecture that allows the model to weigh the importance of different tokens in the input sequence relative to each other\n",
    "\n",
    "The cat sat ->  get converted into vectors and sent to -> self attention layer ->o/p is another vector this is our contextual embedding\n",
    "[100,010,001]                                                                               [0.8,0.3,0.4 0.4,0.6,0.2 0.3,0.4,0.8]\n",
    "\n",
    "1. Inputs:- Query,Key,Values\n",
    "we will create a model that will compute-> Queries,Keys,Values vector\n",
    "\n",
    "I. Query Vectors (Q):\n",
    "Role: Query vectors represent the token for which we are calculating the attention. They help determine the importance of other tokens in the context of the current token.\n",
    "Importance:\n",
    "Focus Determination: Queries help the model decide which parts of the sequence to focus on for each specific token. By calculating the dot product between a queruector and all key vectors, the model assesses how much attention to give to each token relative to the current token.\n",
    "Contextual Understanding: Queries contribute to understanding the relationship between the current\n",
    "token and the rest of the sequence, which is essential for capturing dependencies and context\n",
    "\n",
    "2. Key Vectors (K):\n",
    "Role: Key vectors represent all the tokens in the sequence and are used to compare with the query vectors to calculate attention scores.\n",
    "Importance:\n",
    "Relevance Measurement: Keys are compared with queries to measure the relevance or compatibility of each token with the current token. This comparison helps in determining how much attention each token should receive.\n",
    "Information Retrieval: Keys play a critical role in retrieving the most relevant information from the sequence by providing a basis for the attention mechanism to compute similarity scores.\n",
    "\n",
    "3. Value Vectors (V):\n",
    "Role: Value vectors hold the actual information that will be aggregated to form the output of the attention mechanism.\n",
    "Importance:\n",
    "Information Aggregation: Values contain the data that will be weighted by the attention scores. The weighted sum of values forms the output of the self-attention mechanism, which is then passed on to the next layers in the network.\n",
    "Context Preservation: By weighting the values according to the attention scores, the model preserves and aggregates relevant context from the entire sequence, which is crucial for tasks like translation, summarization, and more.\n",
    "\n",
    "Input sequence = ['The','Cat','Sat']\n",
    "Embedding size = 4\n",
    "Q,K,V=4\n",
    "\n",
    "1.Token embedding:-\n",
    "        E (the) = [1010] E(Cat) = [0101] E(sat) = [1111]\n",
    "\n",
    "2. Linear transformation:-\n",
    "        We create q,k,v by multiplying the embeddings by learned weights(still it's not learned with back propagation Wq,Wk,Wv will get learned or filled) matrices Wq,Wk,Wv\n",
    "\n",
    "CAT[1 0 0]  dot product with Wq[3*3] we will get Q[- - -]\n",
    "                             Wk[3*3] we will get K[- - -]\n",
    "                             Wv[3*3] we will get V[- - -]\n",
    "Wq=Wk=Wv = I [100]\n",
    "              010\n",
    "              001\n",
    "\n",
    "Q(the) = [1010]*I = [1010]\n",
    "K(the) = [1010]\n",
    "V(the) = [1010]\n",
    "\n",
    "    1.Q(the)=K(the)=V(the) = [1010]\n",
    "    2.Q(cat)=K(cat)=V(cat) = [0101]\n",
    "    3.Q(sat)=K(sat)=V(sat) = [1111]\n",
    "\n",
    "3.Lean the attention scores:-\n",
    "\n",
    "For the token THE\n",
    "Score(Q the, K the)=[1010].[1010] Transpose = 2\n",
    "Score(Q the, K cat)=[1010].[0101] Transpose = 0\n",
    "Score(Q the, K sat)=[1010].[1111] Transpose = 2\n",
    "\n",
    "For the token CAT\n",
    "Score(Q cat, K the)=[0101].[1010] Transpose = 0\n",
    "Score(Q cat, K cat)=[0101].[0101] Transpose = 2\n",
    "Score(Q cat, K sat)=[0101].[1111] Transpose = 2\n",
    "\n",
    "For the token SAT\n",
    "Score(Q sat, K the)=[1111].[1010] Transpose = 2\n",
    "Score(Q sat, K cat)=[1111].[0101] Transpose = 2\n",
    "Score(Q sat, K sat)=[1111].[1111] Transpose = 4\n",
    "\n",
    "4.Scaling:-\n",
    "We take up the scores and scale down by dividing the scores by the sqrt(Dk)=> Dk=4 sqrt(Dk)=2\n",
    "Scaling in the attention mechanism is crucial to prevent the dot  product from growing too large -> Ensure stable gradients during training \n",
    "Dk is large means Dimension getting larger\n",
    "    -Gradient exploding\n",
    "    -Softmax saturation\n",
    "Q=[2341] K1=[1010] K2=[0101]\n",
    "Without scaling:- Q . K1 Transpose = 2*1+*0+4*1+1*0= 6\n",
    "                  Q . K2 Transpose = 2*0+3*1+4*0+1*1=4\n",
    "\n",
    "One is score of 6,4 -> Scaling not applied\n",
    "Softmax(6,4) = E^6/E^6+E^4,E^4/E^6+E^4 -> 1/1+E^-2,1/1+E^2 = [0.88,0.12]\n",
    "\n",
    "Most of the attention weight is assigned to first key vector verylittle to the second vector\n",
    "One of the Property of softmax is softmax(10,1) = [0.99,0.01]\n",
    "\n",
    "With scaling :-\n",
    "    - Compute scaled dot product\n",
    "        [6,4]=>scale=>[6/2,4/2]=[3,2]\n",
    "        softmax(E^3/E^3+E^2,E^2/E^3+E^2)=[0.73,0.27]\n",
    "Here the attention weights are more balanced than un balanced case\n",
    "\n",
    "Summary of Importance\n",
    "Stabilizing Training: Scaling prevents extremely large dot products, which helps in stabilizing the gradients during backpropagation, making the training process more stable and efficient.\n",
    "Preventing Saturation: By scaling the dot products, the softmax function produces more balanced attention weights, preventing the model from focusing too heavily on a single token and ignoring others.\n",
    "Improved Learning: Balanced attention weights enable the model to learn better representations by considering multiple relevant tokens in the sequence, leading to better performance on tasks that require context understanding.\n",
    "Scaling ensures that the dot products are kept within a range that allows the softmax function to operate effectively, providing a more balanced distribution of attention weights and improving the overall learning process of the model.\n",
    "\n",
    "4.Scaling = Sqrt(Dk)=sqrt(4) = 2\n",
    "Scaled score(Q the, K the)=2/2=1\n",
    "Scaled score(Q the, K cat)=0/2=1\n",
    "Scaled score(Q the, K sat)=2/2=1\n",
    "\n",
    "Similarly scaling will be done for all other tokens\n",
    "\n",
    "5.Applying softmax:-\n",
    "it's applied to the scaled scores\n",
    "Attention weights of THE= Softmax(1,0,1)=[0.4223,0.1554,0.4223]\n",
    "Attention weights of CAT= Softmax(0,2,2)=[0.1554,0.4223,0.4223]\n",
    "Attention weights of SAT= Softmax(2,2,4)=[0.2119,0.2119,0.5762]\n",
    "\n",
    "6.Weighted sum of values :-\n",
    "we multiply the attention weights by corresponding value vectors \n",
    "for the token THE = Output(THE) = 0.4223 * V the + 0.1554 * V cat + 0.4223 * V sat \n",
    "                                =0.4223[1010]+0.1554[0101]+0.4223[1111]\n",
    "                                =[0.4223,0,0.4223,0]+[0,0.1554,0,0.1554]+[0.4223,0.4223,0.4223,0.4223]\n",
    "                                =[1.2669,0.9999,1.2669,0.9999] => These are contextual vectors\n",
    "\n",
    "The [1010] =>Self attention => [1.2669,0.9999,1.2669,0.9999]\n",
    "\n",
    "-Q,K,V & (Wq,Wk,Wv) are learned and trained vectors got from back propagation\n",
    "-Attention scores\n",
    "-Scaled\n",
    "-Softmax\n",
    "-Weighted sum of values(softmax * V)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4233a303-bf72-4d18-9b5d-7a14cff020ee",
   "metadata": {},
   "source": [
    "            Multi Head Attention\n",
    "It expands model's abilit to focus on different position of tokens\n",
    "Attention Head #0    Attention Head #1   \n",
    "Q0 . W0 Q              Q1 . W1 Q\n",
    "K0 . W0 K              K1 . W1 K\n",
    "V0 . W0 V              V1 . W1 V\n",
    "\n",
    "And then after this normal score,softmax all will occur and we will get Z0,Z1\n",
    "\n",
    "\n",
    "X(Thinking machine)\n",
    "           |\n",
    "           | caluclating attention in eight diff attention heads\n",
    "           |\n",
    "Attention Head #0    Attention Head #1   ..........    Attention Head #7    \n",
    "Q0 . W0 Q              Q1 . W1 Q                          Q7 . W7 Q\n",
    "K0 . W0 K              K1 . W1 K                          K7 . W7 K\n",
    "V0 . W0 V              V1 . W1 V                          V7 . W7 V\n",
    "\n",
    "After self attention we will get all the Z0,Z1,..Z7(Attention head's 1 to 7) ( Multi head attention) and sent to Feed forward neural network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf4a44fa-b8f2-4891-9669-8279669cb664",
   "metadata": {},
   "source": [
    "                Feed forward neural network with multi head attention\n",
    "These 3 steps for 1 word 'The'\n",
    "1.concatinate all the attention heads\n",
    "2.Multiply with a weight matrix W0 that was trained jointly with the model\n",
    "3.The result would be Z matrix that captures information from all the attention heads. We can send this forward to the Feed forward neural network\n",
    "\n",
    "Have to do same for all remaning 'Cat', 'Sat'\n",
    "\n",
    "Final picture in blog - That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8b0b49c-5ef1-4199-ab1f-cbec2686ae7f",
   "metadata": {},
   "source": [
    "                            Positional Encoding - Represent order of sequence\n",
    "Advantage :- Word token it can process parallely\n",
    "specific drawback because of this disadvantage :- Lack the sequential structure of the words {order}\n",
    "Lion kills tiger\n",
    "Tiger kills lion\n",
    "Attention is all you need\n",
    "Add positional vector to x1 and then send to self attention layer \n",
    "\n",
    "                Types of positional encoding\n",
    "1.Sinusoidal positional encoding\n",
    "2.Learned positional encoding => PE are learned during training by back propagation\n",
    "\n",
    "Sinusoidal positional encoding :- It uses sign and cosine functions of different frequencies to create positional encodings\n",
    "all values ranging between -1 to +1 \n",
    "\n",
    "P.E(Pos,2i)=sin(pos/10000 ^2i/D model)\n",
    "P.E(Pos,2i+1) = cos(pos / 10000 ^ 2i/D model)\n",
    "Where pos = position\n",
    "      i = dimension\n",
    "      D model = dimenonality of the embeddings\n",
    "Eg :- The cat sat\n",
    "the -> [0.1 0.2 0.3 0.4]\n",
    "cat -> [0.5 0.6 0.7 0.8]\n",
    "sat -> [0.9 1.0 1.1 1.2]\n",
    "\n",
    "P.E (Pos,2i) = Sin(Pos / 10000 ^ 2i/ D model)\n",
    "P.E(Pos,2i+1) = cos(pos / 10000 ^ 2i/D model)\n",
    "For example d model = 4 (Dimensonality)\n",
    "For position pos=0\n",
    "P.E (0,0) = Sin(0/ 10000 ^ (0/4)) = sin(0) = 0\n",
    "P.E (0,1) = cos(0/ - ) = cos(0) = 1\n",
    "why to use cosine insted use sine we may miss the order we done is added another cosine if we done one with sine another we will be doing with cosine\n",
    "P.E(0,2)=sin(0/ 10000 ^ 2/4) = sin(0) = 0\n",
    "p.e(0,3)=cos(0) = 1\n",
    "\n",
    "P.E = [0,1,0,1]\n",
    "For pos=1\n",
    "P.E(1,0) = Sin(1 / 10000 ^ 0/4 = sin(1) = 0.8415\n",
    "P.E(1,1) = cos(1 / 10000 ^ 2/4 = 0.5403\n",
    "P.E(1,2) = Sin(1 / 10000 ^ 2/4 =0.01\n",
    "P.E(1,3) = cos(1 / 10000 ^ 0/4 =0.9995\n",
    "converted positional encoding is\n",
    "     Vector              Positional encoding\n",
    "The [0.1 0.2 0.3 0.4] -> [0 1 0 1]\n",
    "Cat [0.5 0.6 0.7 0.8] -> [0.8415 0.5403 0.01 0.9995]\n",
    "\n",
    "                    Z1           Z2 -> Context\n",
    "                [Feed forward neural network] -> \n",
    "                        |        |   -> Multihead Attention\n",
    "                     [Self attention]\n",
    "                        |        |\n",
    "        [0.84 0.54 0.01 0.99]  [0 1 0 1]<-Positional encoding\n",
    "                    +              +\n",
    "            [0.5 0.6 0.7 0.8]   [0.1 0.2 0.3 0.4]\n",
    "                    |                  |\n",
    "                   CAT                THE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb587b3b-3ed0-4d5a-a560-4032f85c6e38",
   "metadata": {},
   "source": [
    "                                    LAYER NORMALIZATION\n",
    "ADD AND Normalize\n",
    "https://jalammar.github.io/illustrated-transformer/ - see residuals(basically give additional information) & add and norm\n",
    "add and mormalize is the place where layer normalization occurs\n",
    "\n",
    "Normalization-\n",
    "                1.Batch Normalization\n",
    "                2.Layer Normalization\n",
    "F1 -> O -> O \\\n",
    "               -> O (O/P)\n",
    "F2 -> O -> O /\n",
    "    I/P    Hidden\n",
    "F1             F2      \n",
    "House size  No of rooms   Prize\n",
    "1200           2           45 L\n",
    "1500           3           70 L\n",
    "2000           3.5         80 L\n",
    "All these info we r having before sending to f1 & f2 we do normalization\n",
    "Normalization :- Standard scaling\n",
    "                                    Z score = Xi - U (mu)  / Sigma  => U(mu)=0 (mean),sigma=1(standard deviation)  f1->f1 |(dash) <= U=0,sigma=1\n",
    "In deep learning -> i/p images -> Min max scaler\n",
    "We are having 0-255 pixels for every box in image after min max scaler we will have all in between 0-1\n",
    "\n",
    "Advantages:- 1.Improved training stability -> We willnot face vanishing and exploding gradient problem\n",
    "             2.we will be able to get faster convergence -> when we apply back propagation there will be stable update\n",
    "                z1\n",
    "F1 0.45 -> O -> O \\\n",
    "                    -> O (O/P)\n",
    "F2 0.55 -> O -> O /\n",
    "       I/P    Hidden\n",
    "               z2\n",
    "z1=sigma[(0.45*w1 + 0.55*w3 ) + b1] = value1\n",
    "z2=   -         -        -          = value2\n",
    "\n",
    "F1             F2                  Batch normalization-for tis both z1 and z2 we perform normalization for evey o/p\n",
    "House size  No of rooms   Prize    Z1  Z2\n",
    "0.45           0.55         45 L   -   -\n",
    "0.60           0.20          -     -   -\n",
    " -              -            -     -   -\n",
    "Again we will get U1,sigma1 for z1 & U2 and sigma2 for z2\n",
    "again after these we apply with same Z score we will see finally U=0 (mu)mean sigma=1(standard deviation)\n",
    "\n",
    "        Diff b/w batch & layer normalization :-\n",
    "F1 F2 Z1(o/p) Z2(o/p)\n",
    "-  -   -       -\n",
    "-  -   -       - \n",
    "Batch normalization says take each value n then like whole z1 & z2\n",
    "Layer normalization says take layer by layer like \n",
    "\n",
    "Z1     Z2\n",
    "-      -  -> U1sigma1  Zscore = Xi - U1 / Sigma1        /\n",
    "-      -  -> U2sigma2  Zscore = Xi - U2 / Sigma2        / Layer normalization\n",
    "-      -  -> U3sigma3  Zscore = Xi - U3 / Sigma3        /\n",
    "\n",
    "iN ANN we use batch normalization but in transformers we use layer normalization\n",
    "\n",
    "Learnable parameters Gamma,Beta\n",
    "Z1    Z2\n",
    "-     -\n",
    "-     -\n",
    "0     -\n",
    "0     0  U=0,Sigma=0 layer normalization , no much impact but in batch whole values will change\n",
    "0     0\n",
    "-     0\n",
    "-     -\n",
    "\n",
    "Normalization:-Gamma,Beta -> Learnable params\n",
    "Z1=Sigma[W1 ^ T X + b1]\n",
    "Y=Gamma[Z1-U1 / Sigma 1] + Beta\n",
    "Gamma,Beta ->scale & shift params\n",
    "\n",
    "Example:-\n",
    "1.CAT=[2.0 4.0 6.0 8.0]\n",
    "2.Parameters = gamma=[1.0,1.0,1.0,1.0] -> learned scale\n",
    "                beta=[0,0,0,0] -> shift\n",
    "3.compute the mean:-\n",
    "    U=1/4[2+4+6+8]=20/4=5\n",
    "-compute variance=sigma ^ 2= 1/4[(2-5)^2 + (4-5)^2 +(6-5)^2 + (8-5)^2]=5\n",
    "-Normalize inputs:-\n",
    "        X1 cap= Xi-U/sqrt(Sigma^2 +E) E=le^-5=>avoid division by 0\n",
    "        sqrt(sigma^2+E)=sqrt(5+le^-5)=sqrt(5.00001)=2.236\n",
    "\n",
    "X1^ Cap= 2-5/2.236 = -1.36\n",
    "X2^ Cap= 4-5/2.236 = -0.45\n",
    "X3^ Cap= 6-5/2.236 =  0.45\n",
    "X4^ Cap= 8-5/2.236 =  1.34\n",
    "\n",
    "Normalized vector= X ^ cap=[-1.34 -0.45 0.45 1.34]\n",
    "\n",
    "4.scale and shift\n",
    " original :-    gamma=[1.0,1.0,1.0,1.0] -> learned scale\n",
    "                beta=[0,0,0,0] -> shift\n",
    "after scaling and shifting we will get same original\n",
    "Yi=Gamma i*X^i + Beta i \n",
    "Y=[-1.34 -0.45 0.45 1.34]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c18588b0-ffcf-417b-b1cf-466a94d5b9fe",
   "metadata": {},
   "source": [
    "                Complete Encoder Architecture\n",
    "\n",
    "I/P Sequence -> Text embedding+Pos encoding(512 dimension (based on research paper)) -**-> Multi head Attention (Z1,Z2,Z3,..-8)(R.P)-> Add and then normalize (Add ** residuals imp of residuals) -> Feed forward neural network (512 layers to the o/p)\n",
    "Every word converted to 512 dimensional vectors\n",
    "Q,K,V=64 dimensions as per the research paper\n",
    "why 512 -> seq2seq is more complex\n",
    "embedding vectors=512\n",
    "    Q,K,V=64\n",
    "    Head attention=8\n",
    "\n",
    "Residual connections (Skip connection NN):-\n",
    "        1.Addressing the vanishing gradient problem\n",
    "        -residuals:- create a short paths for gradient to flow directly through the network.gradients sufficiently large\n",
    "        2.Improves gradient flow:- Converge will be faster if convergence will be faster training will be smoother\n",
    "        3.Enables training of deeper networks :-\n",
    "        4.Feed forward NN :- but to capture more info \n",
    "                -Add non linearity\n",
    "                -Processing each pos independently\n",
    "                -self attention captures rln b/w token\n",
    "FFNN->each token representation independently\n",
    "         |_>Transforming these representation furuthers and allows the model to learn richer representation ->ANN\n",
    "FFNN->Deeper => Adds depth to the model \n",
    "Depth increases -> more learnings from data\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "632e6503-7857-4829-8c7a-f39377fb4391",
   "metadata": {},
   "source": [
    "                            Decoder in transformers\n",
    "The transformer decoder is responsible for generating the output sequence one token at a time, using the encoder's output and the previously generated tokens.\n",
    "1.Masked multihead self attention\n",
    "2.Multihead attention [Encoder Decoder attention]\n",
    "3.Feed forward neural network\n",
    "\n",
    "x1,x2,x3(all at once) -> Encoder -> Decoder we will get o/p as y1,y2,y3 for t=1,t=2,t=3\n",
    "\n",
    "-Taining mechanism\n",
    "-Inference mechanism\n",
    "\n",
    "Masked multi head self attention:-\n",
    "    1.I/P embedding and positional embedding -> zero padding -> sequence length equal\n",
    "    2.Linear projection for Q,K,V\n",
    "    3.Scaled dot product attention\n",
    "    4.Mask application }-> Try to understand the imp a.Lookahead mask b.padding mask\n",
    "    5.Multi head attention\n",
    "    6.Concatenation and final linear projection\n",
    "    7.residual connection and layer normalization\n",
    "\n",
    "Dataset\n",
    "Eng text    to   hindi text\n",
    "<x1,x2,x3>        <y1,y2>\n",
    " use zero padding to make same size <y1,y2,0>\n",
    "\n",
    "I/P              O/P\n",
    "[4 5 6 7]       [1 2 3]\n",
    "                [1 2 3 0]\n",
    "Masked multihead attention\n",
    " 1.I/P embedding and positional embedding -> zero padding -> sequence length equal\n",
    "        Output embedding :- [Step 1]\n",
    "            [0.1 0.2 0.3 0.4],\n",
    "            [0.5 0.6 0.7 0.8], + P.E=>0\n",
    "            [0.9 1.0 1.1 1.2],\n",
    "            [0.0 0.0 0.0 0.0]\n",
    "[Step 2]:- Linear projection for Q,K,V  WQ=WK=WV=I identity matrix\n",
    "Create Query Q,key K, value V.\n",
    "Q=Output embedding + Wq=Output embedding\n",
    "K=Output embedding + Wk=Output embedding\n",
    "V=Output embedding + Wv=Output embedding\n",
    "      Q=K=V=[0.1 0.2 0.3 0.4],\n",
    "            [0.5 0.6 0.7 0.8],\n",
    "            [0.9 1.0 1.1 1.2],\n",
    "            [0.0 0.0 0.0 0.0]\n",
    "Step 3: scaled dot product attention caluclation\n",
    "    Scores=Q*K^t/sqrt(d^k)\n",
    "          =Q*K^t/2\n",
    "\n",
    "[0.1 0.2 0.3 0.4],   [0.1]K^t  same for 0.5     0.9   0\n",
    "[0.5 0.6 0.7 0.8], * [0.2]              0.6  &  1.0 & 0\n",
    "[0.9 1.0 1.1 1.2],   [0.3]              0.7     1.1   0\n",
    "[0.0 0.0 0.0 0.0]    [0.4]              0.8     1.2   0\n",
    "\n",
    "[0.1*0.1+0.2*0.2+0.3*0.3+0.4*0.4,\n",
    "0.1*0.5+0.2*0.6+0.3*0.7+0.4*0.8,\n",
    "0.1*0.9+0.2*1.0+0.3*1.1+0.4*1.2,\n",
    "0.1*0+0.2*0+0.3*0+0.4*0]\n",
    "After summing all the below 4 we will get scores\n",
    "Sores=[0.3 0.7 1.1 0.0]\n",
    "\n",
    "Score:-[[0.3 0.7 1.1 0.0]\n",
    "        [0.7 1.9 3.1 0.0]\n",
    "        [1.1 3.1 5.1 0.0]\n",
    "        [0.0 0.0 0.0 0.0]]\n",
    "\n",
    "- Masked application :- It helps manage the structure of the sequences being processed and ensures the model behaves correctly during training and infering\n",
    "\n",
    "reason:- 1.To handle sequence of different lenght in batch\n",
    "         2.To ensure that padding tokens which are added to make sequence of uniform lenght, donot affect the model prediction.\n",
    "Ex:- Sequence 1 [1,2,3]\n",
    "     Sequence 2 [4,5,0] 0 is padding token -> influence the attention mechanism -> lead to incorrect or biased predictions - we will do padding mask (tokens are ignored)\n",
    "Masking:- 1.Padding mask \n",
    "          2.Look ahead mask\n",
    "-Padding mask for seq 1 = [1 1 1] seq 2 = [1 1 0]\n",
    "-In lookahead mask it maintain autoregressive property\n",
    "        Input one by one -> Encoder -> Decoder -> O/P one by one\n",
    "    1.To ensure that each position in the decoder output sequence can only amend to the previous position but no future position\n",
    "    2.sequence -> Language modelling,translation\n",
    "        eg:- [4 5 0] -> [1 1 0]--convert 1d to 2d mask -> For each token in the sequence mask should indicate which token it should attend to\n",
    "            token 1 attends to token 1,2->[1 1 0]\n",
    "                                          [1 1 0]\n",
    "                                          [0 0 0]\n",
    "- Lookahead mask:- Decoder o/p\n",
    "[[100]\n",
    "  [110]\n",
    "  [111]]\n",
    "Combine padding and looking ahead mask:- Element wise multiplication of 2 mask\n",
    "Combine mask =[[1 0 0]  \n",
    "                [1 1 0]\n",
    "                [0 0 0]]\n",
    "\n",
    "Caluclate mask for this                 to get look ahead mask\n",
    "[[0.3 0.7 1.1 0.0]                      [[1 0 0 0]\n",
    " [0.7 1.9 3.1 0.0]                      [1 1 0 0]\n",
    " [1.1 3.1 5.1 0.0]                      [1 1 1 0]\n",
    " [0.0 0.0 0.0 0.0]]                     [1 1 1 1]]\n",
    "\n",
    "Padding mask[Extended to 2d format]\n",
    "[[1 1 1 0]\n",
    " [1 1 1 0]\n",
    " [1 1 1 0]\n",
    " [0 0 0 0]]\n",
    "Combine mask- Lookahead mask + padding mask\n",
    "[1*1, 0*1, 0*1, 0*0]     [[1 0 0 0]   \n",
    "[1*1, 1*1, 0*1, 0*0]   =  [1 1 0 0]  =>replace all 0's with - infinity\n",
    "[1*1, 1*1, 1*1, 0*0]      [1 1 1 0]  \n",
    "[1*1, 1*1, 1*1, 1*0]]     [1 1 1 0]\n",
    "we r not replacing 0 with - infinity it is getting added\n",
    "Masked score:-\n",
    "\n",
    "Where ever there are 0's we can add something\n",
    "[0.3 -ininity -@ -@]\n",
    "[0.7  1.9  -@  -@]\n",
    "[1.1  3.1  5.1 -@]\n",
    "[0     0    0  -@] zero out the influence when the softmax is applied\n",
    "\n",
    "Softmax:- \n",
    "softmax Score =softmax(masked scores)\n",
    "      =[1.0 0 0 0]\n",
    "       [0.3 0.7 0 0]\n",
    "       [0.1 0.3 0.6 0]\n",
    "       [1.0 0 0 0]\n",
    "* weigh sum of value\n",
    "Attention o/p = softmax scores * v\n",
    "\n",
    "Masking in the transformer architecture is essential for several reasons. It helps manage the structure of the sequences being processed and ensures the model behaves correctly during training and inference. Here are the key reasons for using masking:\n",
    "I. Handling Variable-Length Sequences with Padding Mask\n",
    "Purpose\n",
    "To handle sequences of different lengths in a batch.\n",
    "To ensure that padding tokens, which are added to make sequences of uniform length, do not affect the model's predictions.\n",
    "2. Maintaining Autoregressive Property with Look-Ahead Mask\n",
    "Purpose\n",
    "To ensure that each position in the decoder output sequence can only attend to previous\n",
    "positions and itself, but not future positions.\n",
    "This is crucial for sequence generation tasks like language modeling and translation, where the model should not have access to future tokens when predicting the current token."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8b5164a-33fb-4fc5-8650-9589502d7ad0",
   "metadata": {},
   "source": [
    "                ENCODER DECODER MULTIHEAD ATTENTION\n",
    "1.Encoder o/p -> set of attention vectors k&v\n",
    "2.Masked multihead -> attention vector Q\n",
    "These are to be used by each decoder in it's 'encoder - decoder' in it's 'encoder-decoder' attention layer\n",
    "helps the decoder to focus on appropriate places in the i/p sequence\n",
    "in decoder encoder-decoder layer inputs we get are k,v and q as query from decoder only"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4d513f2-68d1-45ca-9323-31b7c915dd2d",
   "metadata": {},
   "source": [
    "                    Final layer(Linear & softmax)\n",
    "vector -> word(o/p)\n",
    "linear layer(fully connected) -> softmax layer\n",
    "linear layer is simple fully connected neural n/w that projects the vector produced by stack of decoders\n",
    "model -> 10,000 -> vocabulary -> logit vectors = 10,000 cells wide\n",
    "softmax - multiclass classification\n",
    "\n",
    "2.softmax layers turns those score into probabilities(all add upto 1)\n",
    "the cell with the height probability is chosen, and the word associated with it is produced as the o/p\n",
    "\n",
    "Recap of training:-\n",
    "output vocabulary:-\n",
    "            Word     a     am     I   thanks  student  <eos>\n",
    "            Index    0      1     2     3        4       5\n",
    "OHE of word 'am'     0      1     0     0        0       0\n",
    "we will get diff wrong values for untrained model thenfind loss fun -> back propagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
